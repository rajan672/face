<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
	<title>Document</title>
	<style>
		body{
			margin: 0px;
			padding: 0px;
			box-sizing: border-box;
			height: 100vh;
			width: 100vw;
			display: flex;
			align-items: center;
			justify-content: center;
		}
		canvas{
			position: absolute;
		}
	</style>
</head>
<body>
	<video src="" id="video" height="500" width="500" autoplay muted></video>
	<script src="face-api.min.js"></script>
	<script>
		//selscting video tag with id
		const video = document.getElementById("video");

		//face api uses promises. here we have to include all the model of face api.
		Promise.all([
			faceapi.nets.tinyFaceDetector.loadFromUri("models"), //it will detect the face 
			faceapi.nets.faceLandmark68Net.loadFromUri("models"), //it will detect the landmark i.e. identifying key points on a face.
			faceapi.nets.faceRecognitionNet.loadFromUri("models"), //matching a human face from a digital image.
			faceapi.nets.faceExpressionNet.loadFromUri("models") // it will detect the expression of the humsn face.
		]).then(startVideo); // it shows 1st promise will be executed then only the function startvideo will run.// 1st promises will run then only video start running.

		//the function startvideo will start the video
		function startVideo() {
			navigator.getUserMedia(   //getusermedia will provide the video media.
				{ video: {} },    //a object video with empety object
				stream => (video.srcObject = stream), // it will give the video on screen
				err => console.error(err) //if the vido will not run then it will give the error
			);
		}
		//to check we will call the functiom i.e. startvideo();

		video.addEventListener("playing", () => {  //event is playing i.e. when the start playing then the code inside will run.
			const canvas = faceapi.createCanvasFromMedia(video);
			document.body.append(canvas); // apending Canvas in the body.

			const displaySize = { width: video.width, height: video.height };
			faceapi.matchDimensions(canvas, displaySize); //it will mark the height and width of the canvas a/c to given video height and width.

			//setting the invwrval of time 
			setInterval(async () => {
				const detections = await faceapi
					.detectAllFaces(video, new faceapi.TinyFaceDetectorOptions()).withFaceLandmarks().withFaceExpressions();
				const resizedDetections = faceapi.resizeResults(detections, displaySize);

				// it will provide a 2d  view using canvas. and clear all the  multiple draw of face detection,face landmark & expression.
				canvas.getContext("2d").clearRect(0, 0, canvas.width, canvas.height);

				//if u want to check then,use one by one faceapi..
				faceapi.draw.drawDetections(canvas, resizedDetections); //it will draw the face which has been detected.
				faceapi.draw.drawFaceLandmarks(canvas, resizedDetections); //it will draw the landmark of the face.
				faceapi.draw.drawFaceExpressions(canvas, resizedDetections); //it will  write the detected expression.
			}, 100); //it will detect an interval of 100 millisecond.
		});
	</script>
</body>
</html>